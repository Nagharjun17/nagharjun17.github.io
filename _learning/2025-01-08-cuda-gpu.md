---
layout: default
title: "CUDA Kernel Microbenchmarks: Naive and Tiled MatMul using RTX 3060"
year: 2025
stack: "C++ • CUDA 12.x • RTX 3060"
excerpt: "Benchmarking and comparing Matrix Multiplication operation on shared memory and naive approach."
tags: [CUDA, GPU]
image: assets/img/projects/cuda-matmul.png
---

{% if page.image %}
<figure>
  <img src="{{ page.image | relative_url }}" alt="{{ page.title }} screenshot"
     loading="lazy" width="640" style="height:auto;">
  <figcaption>{{ page.title }}</figcaption>
</figure>
{% endif %}

**YOU CAN REPLICATE THIS PROJECT -> https://github.com/Nagharjun17/CUDA-Custom-Kernels**

* Implemented custom CUDA kernels (vec_add, matmul_naive, matmul_tiled) with a C++ timing harness using CUDA events (RTX 3060, CUDA 12.x).
* Achieved ~1.3× speedup with shared-memory tiling (e.g., 1024³: 786 → 1038 GFLOPs; 4096³: 839 → 1116 GFLOPs).
* Automated benchmarking pipeline with CSV logging and performance plots (GFLOPs vs. size).
* Profiled kernels using Nsight Compute to analyze occupancy and memory coalescing.

[GitHub Repo](https://github.com/Nagharjun17/CUDA-Custom-Kernels)

<div style="margin-top: 2rem;">
  <a href="/learning" style="text-decoration: none; font-weight: bold;">← Back</a>
</div>
