---
layout: default
title: "MLIR to PTX: Custom Dialect Fusion and GPU Lowering"
year: 2025
stack: "C++ • MLIR • LLVM • CUDA"
excerpt: "Tiny playground for a custom MLIR dialect with Add+ReLU fusion and lowering to PTX."
tags: [MLIR, LLVM, PTX, CUDA]
image: assets/img/projects/ptx.png
---

{% if page.image %}
<figure>
  <img src="{{ page.image | relative_url }}" alt="{{ page.title }} screenshot"
     loading="lazy" width="640" style="height:auto;">
  <figcaption>{{ page.title }}</figcaption>
</figure>
{% endif %}

**YOU CAN REPLICATE THIS PROJECT -> https://github.com/Nagharjun17/MLIR-to-PTX-CUDA**

* Built a custom MLIR dialect (**mcomp**) with a fused op `mcomp.fuse_add_relu`.
* Implemented passes: fuse arith.addf and relu to mcomp.fuse_add_relu, and lower back to arith.
* Extended lowering pipeline: MLIR → LLVM IR → PTX (via NVVM).
* Verified outputs by compiling test MLIR files to LLVM IR and GPU PTX assembly.
* Demonstrated PTX kernel emission for NVIDIA GPUs (RTX 3060).

[GitHub Repo](https://github.com/Nagharjun17/MLIR-to-PTX-CUDA)

<div style="margin-top: 2rem;">
  <a href="/learning" style="text-decoration: none; font-weight: bold;">← Back</a>
</div>
